<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="description" content="Second Workshop on Scholarly Document Processing">

  <title>2nd Workshop on Scholarly Document Processing</title>

  <!-- Bootstrap core CSS -->
  <link href="./dist/css/bootstrap.min.css" rel="stylesheet">

  <!-- Fira Sans font -->
  <link href="https://fonts.googleapis.com/css?family=Fira+Sans&display=swap" rel="stylesheet">

  <!--[if lt IE 9]><script src="../../assets/js/ie8-responsive-file-warning.js"></script><![endif]-->

  <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
  <!--[if lt IE 9]>
      <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
      <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->

  <!-- Custom styles for this template -->
  <link href="styles.css" rel="stylesheet">

  <!-- icons -->
  <link rel="stylesheet" href="./font-awesome-4.1.0/css/font-awesome.min.css">

  <!-- jQuery -->
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.12.4/jquery.min.js"></script>

</head>

<body>

  <!-- NAVBAR ================================================== -->

  <div class="navbar-wrapper"></div>
  <script src="menu.js" type="text/javascript"></script>

  <!-- MAIN CONTENT ============================================= -->

  <div class="container marketing navbar-spacing">

    <div class="row">
      <div class="col-md-12">

        <!-- CFP INTRODUCTION ================================================== -->

        <!-- <h1>The 6<sup>th</sup> Computational Linguistics Scientific Document Summarization Shared
          Task (CL-SciSumm 2020)</h1> -->

        <h1>Shared Tasks: Call for Participation</h1>

        <!-- <p>
          <a href="https://docs.google.com/forms/d/e/1FAIpQLScfHzByrog-k299qBuCp3SbPWcb905_kmOWMvHpDH57VLpVrg/viewform"><button type="button" class="btn btn-primary">Shared Tasks Registration</button></a>
        </p>

        <hr class="featurette-divider">

        <h2>Navigation</h2>

        <ul>
          <li>
            <a href="#clscisumm">CL-SciSumm 2020</a>
          </li>
          <li>
            <a href="#laysumm">CL-LaySumm 2020</a>
          </li>
          <li>
            <a href="#longsumm">LongSumm 2020</a>
          </li>
          <li>
            <a href="#register">Registration</a>
          </li>
          <li>
            <a href="#dates">Important Dates</a>
          </li>
          <li>
            <a href="#organizers">Organizing Committee</a>
          </li>
        </ul> -->

        <hr class="featurette-divider">

        <!-- LongSumm ========================================================= -->

        <h2 id="longsumm">LongSumm 2021: The 2<sup>nd</sup> Shared Task on Generating Long Summaries for Scientific
          Documents</h2>

        <p>
          The LongSumm Shared Task focuses on generating long summaries of scientific articles which require expertise
          in a scientific domain. To this end, we created a training set that consists of 1,705 extractive summaries,
          and 700 abstractive summaries of NLP and Machine Learning scientific papers. These are drawn from papers based
          on video talks from associated conferences (<a href="https://www.aclweb.org/anthology/P19-1204/">TalkSumm</a>)
          and from blogs created by NLP and ML researchers. Each submission is judged against one reference summary
          (gold summary) on ROUGE and should not exceed 600 words.
        </p>

        <p>In SDP 2020, LongSumm received 100 submissions from 10 different teams. Evaluation results are reported on a
          <a href="https://aieval.draco.res.ibm.com/challenge/39/leaderboard/39">public leaderboard</a>. In 2021, the
          task will continue to expand, by incorporating additional summaries.</p>

        <!-- <p>
          Most of the work on scientific document summarization focuses on generating relatively short summaries (abstract like). While such a length constraint can be sufficient for summarizing news articles, it is far from sufficient for summarizing scientific work. In fact, such a short summary resembles more to an abstract than to a summary that aims to cover all the salient information conveyed in a given text. Writing such summaries requires expertise and a deep understanding in a scientific domain, as can be found in some researchers blogs.
        </p>

        <p>
          The LongSumm task opted to leverage blogs created by researchers in the NLP and Machine learning communities and use these summaries as reference summaries to compare the submissions against.
        </p>

        <p>
          The corpus for this task includes a training set that consists of 1,705 extractive summaries, and around 700 abstractive summaries of NLP and Machine Learning scientific papers. These are drawn from papers based on video talks from associated conferences (<a href="https://arxiv.org/abs/1906.01351">Lev et al. 2019 TalkSumm</a>) and from blogs created by NLP and ML researchers. In addition, we create a test set of abstractive summaries. Each submission is judged against one reference summary (gold summary) on ROUGE and should not exceed 600 words.
        </p>

        <h3>Long Summary Task</h3>

        <p>
          The task is defined as follows:
        </p>

        <ul>
          <li>
            <strong>Given:</strong> For a detailed description of the provided data, please see the <a href="https://github.com/guyfe/LongSumm#training-data">LongSumm GitHub repository</a>
          </li>
          <li>
            <strong>Task:</strong> Generate abstractive and extractive summaries for scientific papers
          </li>
        </ul>

        <h4>Evaluation</h4>

        <p>
          The Long Summary Task will be scored by using several ROUGE metrics to compare the system output and the gold standard Lay Summary. The intrinsic evaluation will be done by ROUGE, using ROUGE-1, -2, -L and Skipgram metrics. In addition, a randomly selected subset of the summaries will undergo human evaluation.
        </p>

        <h3>Corpus</h3>

        <p>
          The training data is composed of abstractive and extractive summaries. To download both datasets, and for further details, see the <a href="https://github.com/guyfe/LongSumm#training-data">LongSumm GitHub repository</a>.
        </p>

        <p>
          The (blind) test dataset will be released on July 15, 2020.
        </p>-->

        <h3>Organizers</h3>

        <p><a href="https://researcher.watson.ibm.com/researcher/view.php?person=il-GUYF">Guy Feigenblat</a>, IBM Research AI</p>

        <p><a href="https://researcher.watson.ibm.com/researcher/view.php?person=il-SHMUELI">Michal Shmueli-Scheuer</a>, IBM Research AI</p>

        <hr class="featurette-divider">

        <!-- SciFact ========================================================= -->

        <h2 id="scifact">Shared Task on Rationalized Verification of Scientific Claims</h2>

        <p>
          To help scientists with information overload and support scientific fact checking and evidence synthesis, 
          we aim to build systems that can take an input claim, identify all relevant Supporting and Refuting text 
          excerpts from papers in a large corpus, and provide the rationales for those predictions. In this shared task, 
          we will use the <a href="https://arxiv.org/abs/2004.14974">SciFact
            dataset</a> dataset of 1.4K expert-annotated biomedical claims verified against a large corpus of peer-reviewed papers. 
            The <a href="https://scifact.apps.allenai.org/leaderboard">public leaderboard</a> already has submissions making progress on this task.
          We will augment a subset of the SciFact corpus with additional annotations of <a href="https://arxiv.org/abs/1702.05398">
              scientific discourse segments</a>, which will serve as a more detailed form of rationales for verified claims, and 
          can potentially help identify new claims as well.
        </p>

        <h3>Organizers</h3>

        <!-- <p><a href="http://armancohan.com/">Arman Cohan</a>, Allen Institute for Artificial Intelligence (AI2)</p> -->

        <p><a href="https://kyleclo.github.io/">Kyle Lo</a>, Allen Institute for Artificial Intelligence (AI2)</p>

        <p><a href="https://beltagy.net/">Iz Beltagy</a>, Allen Institute for Artificial Intelligence (AI2)</p>

        <p><a href="https://www.linkedin.com/in/anitadewaard/">Anita de Waard</a>, Elsevier, USA</p>

        <p><a href="https://tirthankarslg.wixsite.com/ainlpmldl">Tirthankar Ghosal</a>, Indian Institute of Technology Patna, India</p>

        <p><a href="http://kmi.open.ac.uk/people/member/petr-knoth">Petr Knoth</a>, The Open University, UK</p>

        <!-- <p><a href="https://llwang.net/">Lucy Lu Wang</a>, Allen Institute for Artificial Intelligence (AI2)</p> -->

        <hr class="featurette-divider">

        <!-- 3C ======================================================== -->

        <h2 id="3c">3C Citation Context Classification</h2>

        <p>Recent years have witnessed a massive increase in the amount of scientific literature and research data being
          published online, providing revelation about the advancements in the field of different domains. The introduction of
          aggregator services like CORE <a href="#references">[1]</a> has enabled unprecedented levels of open access to
          scholarly publications. The availability of full text of the research documents facilitates the possibility of
          extending the bibliometric studies by identifying the context of the citations <a href="#references">[2]</a>. The
          shared task organized as part of the WOSP 2020 focuses on classifying citation context in research publications based
          on their influence and purpose.</p>
        
        <p><strong>Subtask A:</strong> A task for indentifying the purpose of a citation. Multiclass classification of citations into one of six classes: Background, Uses,
          Compare_Contrast, Motivation, Extension, and Future.</p>

        <p><strong>Subtask B:</strong> A task for identifying the importance of a citation. Binary classification of citations into one of two classes: Incidental, and
          Influential.</p>
        
        <h3>Dataset</h3>

        <p>The participants will be provided with a labeled dataset of 3000 instances annotated using the ACT platform <a
            href="#references">[3]</a>.</p>
        <p>The dataset is provided in csv format and contains the following fields:</p>
        
        <ul>
          <li>Unique Identifier</li>
          <li>COREID of Citing Paper</li>
          <li>Citing Paper Title</li>
          <li>Citing Paper Author</li>
          <li>Cited Paper Title</li>
          <li>Cited Paper Author</li>
          <li>Citation Context</li>
          <li>Citation Class Label</li>
          <li>Citation Influence Label</li>
        </ul>
        
        <p>Each citation context in the dataset contains an "#AUTHOR_TAG" label, which represents the citation that is being
          considered. All other fields in the dataset correspond to the values associated with the #AUTHOR_TAG. The possible
          values of the <strong>citation_class_label</strong> are:</p>
        
        <ul>
          <li>0 - BACKGROUND</li>
          <li>1 - COMPARES_CONTRASTS</li>
          <li>2 - EXTENSION</li>
          <li>3 - FUTURE</li>
          <li>4 - MOTIVATION</li>
          <li>5 - USES</li>
        </ul>
        
        <p>and that of <strong>citation_influence_label</strong> are:
        
        <ul>
          <li>0 - INCIDENTAL</li>
          <li>1 - INFLUENTIAL</li>
        </ul>
        
        </p>
        <p>The following table shows a sample entry from the training dataset.</p>
        
        <table class="table table-striped">
          <tr>
            <td>unique_id</td>
            <td>1998</td>
          </tr>
          <tr>
            <td>core_id</td>
            <td>81605842</td>
          </tr>
          <tr>
            <td>citing_title</td>
            <td>Everolimus improves behavioral deficits in a patient with autism associated with tuberous
              sclerosis: a case report</td>
          </tr>
          <tr>
            <td>citing_author</td>
            <td>Ryouhei Ishii</td>
          </tr>
          <tr>
            <td>cited_title</td>
            <td>Learning disability and epilepsy in an epidemiological sample of individuals with tuberous
              sclerosis complex</td>
          </tr>
          <tr>
            <td>cited_author</td>
            <td>Joinson</td>
          </tr>
          <tr>
            <td>citation_context</td>
            <td>West syndrome (infantile spasms) is the common estc epileptic disorder, which is associated
              with more intellectual disability and a less favorable neurological outcome (#AUTHOR_TAG et al, 2003)</td>
          </tr>
          <tr>
            <td>citation_class_label</td>
            <td>4</td>
          </tr>
          <tr>
            <td>citation_influence_label</td>
            <td>1</td>
          </tr>
        </table>

        <br />
        <p>The <a href="http://jurgens.people.si.umich.edu/citation-function/" target="_blank">ACL-ARC</a> dataset <a
            href="#references">[4]</a>, which is compatible with our ACT dataset can be used by the participants during the
          competition.</p>
        
        <h3>Evaluation</h3>
        <p>The evaluation will be conducted using the withheld test data containing 1000 instances. The evaluation metric used
          will be the F1-macro score.</p>
        
        <!-- <p style="text-align:center;"><img src="site-assets/images/CodeCogsEqn.gif"></p> -->
        
        $$\mbox{F1-macro} = {\frac{1}{n} \sum_{i=1}^{n}{\frac{2 \times P_i \times R_i}{P_i + R_i}}}$$
        
        <h3 id="registration">Team Registration</h3>

        <p>A registration form and a link to the dataset will be provided shortly.</p>
        
        <!-- <p>This year, we are hosting the shared task on the <a href="https://www.kaggle.com/c/about/inclass"
            target="_blank">Kaggle inClass</a> platform. Please note that both subtasks will be hosted as separate competitions
          on kaggle. Please make sure you sign in/register to kaggle before clicking the following links.</p>
        
        <p>To participate in the Shared Task:</p>
        
        <dl class="topics">
          <dd>For subtask A, please visit <a href="https://www.kaggle.com/c/3c-shared-task-purpose/" target="_blank">Kaggle
              citation purpose </a>classification</dd>
          <dd>For subtask B, please visit <a href="https://www.kaggle.com/c/3c-shared-task-influence/" target="_blank">Kaggle
              citation influence </a>classification</dd>
        </dl> -->

        <!-- <p>
          To address the problem of all citations being treated equally by research metrics, we will run a shared task that will encourage the development of models capable of classifying citations according to their <i>influence</i> and <i>purpose</i>. The shared task will build on our experience from organizing the <a href="https://wosp.core.ac.uk/jcdl2020/shared_task.html#3ctask">3C shared task at WOSP 2020</a>, as well as the <a href="https://biendata.com/competition/wsdm2020/">Citation Intent Classification challenge</a> at WSDM 2020.
        </p>

        <p>
          Three existing datasets will be used: 
        </p>

        <ul>
          <li>the <a href="http://oro.open.ac.uk/70520/">ACT dataset</a> (over 11 thousand author annotated citation contexts),</li>
          <li><a href="https://dl.acm.org/doi/abs/10.1145/2740908.2742839">Microsoft Academic Graph</a> (a dataset of scientific publication records with citation links and contexts) and</li>
          <li>the <a href="http://oro.open.ac.uk/55987/">TrueImpactDataset</a> (a dataset of research papers annotated as seminal or survey).</li>
        </ul>

        <p>
          In addition to subtasks 1 and 2 introduced in 3C 2020, we will include a new subtask focused on classification of publications into seminal and survey categories according to the context in which they are cited. 3C 2020 saw 3 teams participate in subtask 1 and 4 teams in subtask 2, while WSDM Cup 2020 saw 99 teams participate in the challenge.
        </p> -->

        <h3>Organizers</h3>

        <p><a href="http://kmi.open.ac.uk/people/member/petr-knoth">Petr Knoth</a>, Open University, UK</p>

        <p><a href="http://kmi.open.ac.uk/people/member/suchetha-n-kunnath">Suchetha N. Kunnath</a>, Open University, UK</p>

        <p><a href="http://kmi.open.ac.uk/people/member/david-pride">David Pride</a>, Open University, UK</p>

        <!-- <p><a href="https://www.microsoft.com/en-us/research/people/kuansanw/">Kuansan Wang</a>, Microsoft Research</p> -->

        <!-- <p><a href="http://dasha.tech">Drahomira Herrmannova</a>, Oak Ridge National Laboratory</p> -->

        <h3>References</h3>

        <ol id="refs">
          <li>Knoth, Petr and Zdrahal, Zdenek. "<a href="http://www.dlib.org/dlib/november12/knoth/11knoth.html">CORE: three access levels to underpin open access</a>." D-Lib Magazine 18.11/12 (2012): 1-13.</li>
          <li>Pride, David and Knoth, Petr. "<a href="http://oro.open.ac.uk/51751/">Incidental or influential? &mdash; A decade of using text-mining for citation function classification</a>." (2017).</li>
          <li>Pride, David, Knoth, Petr and Harag, Jozef. "<a href="https://ieeexplore.ieee.org/abstract/document/8791134">ACT: An Annotation Platform for Citation Typing at Scale</a>." 2019 ACM/IEEE Joint Conference on Digital Libraries (JCDL). IEEE, 2019.</li>
          <li>Jurgens, David, et al. "<a href="https://www.mitpressjournals.org/doi/abs/10.1162/tacl_a_00028">Measuring the evolution of a scientific field through citation frames</a>." Transactions of the Association for Computational Linguistics 6 (2018): 391-406.</li>
        </ol>

        <!-- <hr class="featurette-divider"> -->

        <!-- IMPORTANT DATES ========================================================= -->

        <!-- <h2 id="register">Registration</h2>

        <p>
          To register for participation in the shared tasks, please use <a href="https://docs.google.com/forms/d/e/1FAIpQLScfHzByrog-k299qBuCp3SbPWcb905_kmOWMvHpDH57VLpVrg/viewform">this registration form</a>.
        </p>

        <p>TBA</p>

        <hr class="featurette-divider"> -->

        <!-- IMPORTANT DATES ========================================================= -->

        <!-- <h2 id="dates">Important Dates</h2>

        <p>
          Please consult the <a href="index.html">SDP Workshop website</a> for official dates for the workshop. All submission deadlines are 11:59 PM AoE (Anywhere on Earth) Time Zone (UTC-12).
        </p>

        <table class="table table-striped">
          <thead>
            <tr>
              <th scope="col">Event</th>
              <th scope="col">Date</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>Training Set Release</td>
              <td>Feb 15, 2020. An additional development set will be made available closer to the test set release date</td>
            </tr>
            <tr>
              <td>Deadline for Registration</td>
              <td> April 30 (remains open till evaluation window starts)<br /></td>
            </tr>
            <tr>
              <td>Test Set Release (Blind)</td>
              <td><del>July 1, 2020</del> July 15, 2020</td>
            </tr>
            <tr>
              <td>System Runs Due</td>
              <td><del>August 1, 2020</del> Aug 15, 2020 </td>
            </tr>
            <tr>
              <td>Preliminary System Reports Due in SoftConf</td>
              <td>August 16, 2020</td>
            </tr>
            <tr>
              <td>Camera-Ready Contributions Due in SoftConf</td>
              <td><del>August 31, 2020</del> Oct 10, 2020</td>
            </tr>
            <tr>
              <td>Participant Presentations at SDP 2020</td>
              <td><del>Nov 12</del> Nov 19, 2020</td>
            </tr>
          </tbody>
        </table>

        <hr class="featurette-divider"> -->

      </div>
    </div>

    <!-- FOOTER ========================================== -->

    <hr><br />

    <footer>
      <div class="footer-wrapper">
        <div class="footer-left">
          <p>Contact: <a href="mailto:sdproc2021@googlegroups.com">sdproc2021@googlegroups.com</a></p>
          <p>Sign up for updates: <a href="https://groups.google.com/g/sdproc-updates">https://groups.google.com/g/sdproc-updates</a></p>
          <p>Follow us: <a href="https://twitter.com/sdproc">https://twitter.com/SDProc</a></p>
        </div>
        <div class="footer-right">
          <a href="#">Back to top</a>
        </div>
    </footer>

  </div>

  <!-- Bootstrap core JavaScript ================================================== -->
  <script src="./dist/js/bootstrap.min.js"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

</body>

</html>