<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="description" content="Second Workshop on Scholarly Document Processing">

  <title>2nd Workshop on Scholarly Document Processing</title>

  <!-- Bootstrap core CSS -->
  <link href="./dist/css/bootstrap.min.css" rel="stylesheet">

  <!-- Fira Sans font -->
  <link href="https://fonts.googleapis.com/css?family=Fira+Sans&display=swap" rel="stylesheet">

  <!--[if lt IE 9]><script src="../../assets/js/ie8-responsive-file-warning.js"></script><![endif]-->

  <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
  <!--[if lt IE 9]>
      <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
      <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->

  <!-- Custom styles for this template -->
  <link href="styles.css" rel="stylesheet">

  <!-- icons -->
  <link rel="stylesheet" href="./font-awesome-4.1.0/css/font-awesome.min.css">

  <!-- jQuery -->
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.12.4/jquery.min.js"></script>

</head>

<body>

  <!-- NAVBAR ================================================== -->

  <div class="navbar-wrapper"></div>
  <script src="menu.js" type="text/javascript"></script>

  <!-- MAIN CONTENT ============================================= -->

  <div class="container marketing navbar-spacing">

    <div class="row">
      <div class="col-md-12">

        <!-- CFP INTRODUCTION ================================================== -->

        <!-- <h1>The 6<sup>th</sup> Computational Linguistics Scientific Document Summarization Shared
          Task (CL-SciSumm 2020)</h1> -->

        <h1>Shared Tasks: Call for Participation</h1>

        <!-- <p>
          <a href="https://docs.google.com/forms/d/e/1FAIpQLScfHzByrog-k299qBuCp3SbPWcb905_kmOWMvHpDH57VLpVrg/viewform"><button type="button" class="btn btn-primary">Shared Tasks Registration</button></a>
        </p>

        <hr class="featurette-divider"> -->

        <h2>Quick links</h2>

        <ul>
          <li>
            <a href="#longsumm">LongSumm: Generating Long Summaries for Scientific Documents</a>
          </li>
          <li>
            <a href="#sciver">SciVer: Verifying Scientific Claims with Rationales</a>
          </li>
          <li>
            <a href="#3c">3C: Citation Context Classification</a>
          </li>
        </ul>

        <hr class="featurette-divider">

        <!-- LongSumm ========================================================= -->

        <h2 id="longsumm">LongSumm 2021: The 2<sup>nd</sup> Shared Task on Generating Long Summaries for Scientific
          Documents</h2>

        <!-- <p>
          The LongSumm Shared Task focuses on generating long summaries of scientific articles which require expertise
          in a scientific domain. To this end, we created a training set that consists of 1,705 extractive summaries,
          and 700 abstractive summaries of NLP and Machine Learning scientific papers. These are drawn from papers based
          on video talks from associated conferences (<a href="https://www.aclweb.org/anthology/P19-1204/">TalkSumm</a>)
          and from blogs created by NLP and ML researchers. Each submission is judged against one reference summary
          (gold summary) on ROUGE and should not exceed 600 words.
        </p> -->

        <p>
          Most of the work on scientific document summarization focuses on generating relatively short summaries (abstract like). While such a length constraint can be sufficient for summarizing news articles, it is far from sufficient for summarizing scientific work. In fact, such a short summary resembles more to an abstract than to a summary that aims to cover all the salient information conveyed in a given text. Writing such summaries requires expertise and a deep understanding in a scientific domain, as can be found in some researchers blogs.
        </p>

        <p>
          The LongSumm task opted to leverage blogs created by researchers in the NLP and Machine learning communities and use these summaries as reference summaries to compare the submissions against.
        </p>

        <p>
          The corpus for this task includes a training set that consists of 1,705 extractive summaries, and around 700 abstractive summaries of NLP and Machine Learning scientific papers. These are drawn from papers based on video talks from associated conferences (<a href="https://arxiv.org/abs/1906.01351">Lev et al. 2019 TalkSumm</a>) and from blogs created by NLP and ML researchers. In addition, we create a test set of abstractive summaries. Each submission is judged against one reference summary (gold summary) on ROUGE and should not exceed 600 words.
        </p>

        <p>This is the second year LongSumm is being hosted &mdash; the results from LongSumm @ SDP 2020 are reported on a 
          <a href="https://aieval.draco.res.ibm.com/challenge/39/leaderboard/39">public leaderboard</a>. In 2021, the
          task will continue to expand, by incorporating additional summaries.</p>

        <h3>Long Summary Task</h3>

        <p>
          The task is defined as follows:
        </p>

        <ul>
          <li>
            <strong>Given:</strong> For a detailed description of the provided data, please see the <a href="https://github.com/guyfe/LongSumm#training-data">LongSumm GitHub repository</a>
          </li>
          <li>
            <strong>Task:</strong> Generate abstractive and extractive summaries for scientific papers
          </li>
        </ul>

        <h3>Evaluation</h3>

        <p>
          The Long Summary Task will be scored by using several ROUGE metrics to compare the system output and the gold standard Lay Summary. The intrinsic evaluation will be done by ROUGE, using ROUGE-1, -2, -L and Skipgram metrics. In addition, a randomly selected subset of the summaries will undergo human evaluation.
        </p>

        <h3>Corpus</h3>

        <p>
          The training data is composed of abstractive and extractive summaries. To download both datasets, and for further details, see the <a href="https://github.com/guyfe/LongSumm#training-data">LongSumm GitHub repository</a>.
        </p>

        <!-- <p>
          The (blind) test dataset will be released on July 15, 2020.
        </p> -->

        <h3>Organizers</h3>

        <p><a href="https://researcher.watson.ibm.com/researcher/view.php?person=il-GUYF">Guy Feigenblat</a>, IBM Research AI</p>

        <p><a href="https://researcher.watson.ibm.com/researcher/view.php?person=il-SHMUELI">Michal Shmueli-Scheuer</a>, IBM Research AI</p>

        <h3>Contact</h3>

        <p>Please contact <a href="mailto:shmueli@il.ibm.com">shmueli@il.ibm.com</a> and <a href="mailto:guyf@il.ibm.com">guyf@il.ibm.com</a> with questions about this shared task.</p>

        <hr class="featurette-divider">

        <!-- SciVer ========================================================= -->

        <h2 id="sciver">SciVer: Scientific claim verification</h2>

        <p>
          Due to the rapid growth in scientific literature, it is difficult for scientists to stay up-to-date on the latest findings. 
          This challenge is especially acute during pandemics due to the risk of making decisions based on outdated or incomplete information. 
          There is a need for AI systems that can help scientists with information overload and support scientific fact checking and evidence synthesis.
        </p>
        
        <p>
          In the SciVer shared task, we will build systems that can:
          <ol>
            <li>Take a scientific claim as input</li>
            <li>Identify all relevant abstracts in a large corpus</li>
            <li>Label them as Supporting or Refuting the claim</li>
            <li>Select sentences as supporting evidence for the label</li>
          </ol>

        Here’s a <a href="https://scifact.apps.allenai.org/">live demo</a> of what such a system would do. 
        </p>
      
        <h3>Dataset</h3>
        <p>
          We will use the <strong>SciFact dataset</strong> of 1,409 expert-annotated biomedical claims verified against 5,183 abstracts from peer-reviewed publications. 
          <strong>Download</strong> the full dataset and any baseline models from <a href="https://github.com/allenai/scifact">GitHub</a>. 
          Find out more from the <a href="https://www.aclweb.org/anthology/2020.emnlp-main.609/">EMNLP 2020 paper.</a> 
        </p>
      
        <p>
          For each claim, we provide:
          <ul>
              <li>A list of abstracts from the corpus containing relevant evidence.</li>
              <li>A label indicating whether each abstract <em>supports</em> or <em>refutes</em> the claim.</li>
              <li>The evidence (or <em>rationales</em>) found in each abstract that justify the label.</li>
            </ul>
        </p>
    
    
         <p>
           An example of a claim paired with evidence from a single abstract is shown below.
            <pre>
            <code>
          {
              "claim": "ALDH1 expression is associated with poorer prognosis for breast cancer primary tumors.",
              "evidence": {
                  "30041340": [
                      { "sentences": [ 0, 1 ],  // Sentences 0 and 1, taken together, support the claim.
                         "label": "SUPPORT" },
                      { "sentences": [ 11 ],    // Sentence 11, on its own, supports the claim.
                         "label": "SUPPORT" }
                  ]
              }
          }
            </code>
           </pre>
        </p>
 
        <h3>Registration</h3>
        <p>
          Please send an email to the organizers at <a href="mailto:sciver-info@allenai.org">sciver-info@allenai.org</a> with:
          <ul>
            <li>Team name</li>
            <li>Participant (full) name(s)</li>
            <li>Participant affiliation(s)</li>
            <li>Email(s) for primary contact(s)</li>
          </ul>
        </p>

        <h3>Timeline</h3>
        <p>
          <ul>
            <li>Train & public test set release – December 14, 2020 (registration opens)</li>
            <li>Blind test set release – February 19, 2021</li>
            <li>Blind test set runs due – February 26, 2021 (registration closes)</li>
            <li>Final evaluation results published – March 1, 2021</li>
            <li>All paper submissions due – March 15, 2021</li>
            <li>Notification of acceptance – April 15, 2021</li>
            <li>Camera-ready papers due – April 26, 2021</li>
            <li>Workshop – June 10, 2020</li>
          </ul>
        </p>
    
        <h3>Organizers</h3>

        <p><a href="https://dwadden.github.io/">Dave Wadden</a>, University of Washington</p>

        <p><a href="https://kyleclo.github.io/">Kyle Lo</a>, Allen Institute for Artificial Intelligence (AI2)</p>

        <p><a href="https://beltagy.net/">Iz Beltagy</a>, Allen Institute for Artificial Intelligence (AI2)</p>

        <p><a href="https://www.linkedin.com/in/anitadewaard/">Anita de Waard</a>, Elsevier, USA</p>

        <p><a href="https://tirthankarslg.wixsite.com/ainlpmldl">Tirthankar Ghosal</a>, Indian Institute of Technology Patna, India</p>

        <!-- <p><a href="http://kmi.open.ac.uk/people/member/petr-knoth">Petr Knoth</a>, The Open University, UK</p> -->

        <!-- <p><a href="https://llwang.net/">Lucy Lu Wang</a>, Allen Institute for Artificial Intelligence (AI2)</p> -->

        <h3>Contact</h3>

        If you have any questions about this shared task, please reach out via email to <a href="mailto:sciver-info@allenai.org">sciver-info@allenai.org</a> 
        or <a href="mailto:dwadden@cs.washington.edu">dwadden@cs.washington.edu</a> and <a href="mailto:kylel@allenai.org">kylel@allenai.org</a>.

        <h3>References</h3>
        <ol>
          <li>Wadden, D., Lin, S., Lo, K., Wang, L.L., Zuylen, M.V., Cohan, A., & Hajishirzi, H. "<a href="https://www.aclweb.org/anthology/2020.emnlp-main.609/">Fact or Fiction: Verifying Scientific Claims</a>." EMNLP (2020).</li>
          <li>Thorne, J., Vlachos, A., Christodoulopoulos, C., & Mittal, A. "<a href="https://www.aclweb.org/anthology/N18-1074/">FEVER: a large-scale dataset for Fact Extraction and VERification</a>." NAACL (2018).</li>
        </ol>
  
        <hr class="featurette-divider">

        <!-- 3C ======================================================== -->

        <h2 id="3c">3C Citation Context Classification</h2>

        <p>Recent years have witnessed a massive increase in the amount of scientific literature and research data being
          published online, providing revelation about the advancements in the field of different domains. The introduction of
          aggregator services like CORE <a href="#references">[1]</a> has enabled unprecedented levels of open access to
          scholarly publications. The availability of full text of the research documents facilitates the possibility of
          extending the bibliometric studies by identifying the context of the citations <a href="#references">[2]</a>. The
          shared task organized as part of the SDP 2021 focuses on classifying citation context in research publications based
          on their influence and purpose.</p>
        
        <p><strong>Subtask A:</strong> A task for identifying the purpose of a citation. Multiclass classification of citations into one of six classes: Background, Uses,
          Compare_Contrast, Motivation, Extension, and Future.</p>

        <p><strong>Subtask B:</strong> A task for identifying the importance of a citation. Binary classification of citations into one of two classes: Incidental, and
          Influential.</p>
        
        <h3>Dataset</h3>

        <p>The participants will be provided with a labeled dataset of 3000 instances annotated using the ACT platform <a
            href="#references">[3]</a>.</p>
        <p>The dataset is provided in csv format and contains the following fields:</p>
        
        <ul>
          <li>Unique Identifier</li>
          <li>COREID of Citing Paper</li>
          <li>Citing Paper Title</li>
          <li>Citing Paper Author</li>
          <li>Cited Paper Title</li>
          <li>Cited Paper Author</li>
          <li>Citation Context</li>
          <li>Citation Class Label</li>
          <li>Citation Influence Label</li>
        </ul>
        
        <p>Each citation context in the dataset contains an "#AUTHOR_TAG" label, which represents the citation that is being
          considered. All other fields in the dataset correspond to the values associated with the #AUTHOR_TAG. The possible
          values of the <strong>citation_class_label</strong> are:</p>
        
        <ul>
          <li>0 - BACKGROUND</li>
          <li>1 - COMPARES_CONTRASTS</li>
          <li>2 - EXTENSION</li>
          <li>3 - FUTURE</li>
          <li>4 - MOTIVATION</li>
          <li>5 - USES</li>
        </ul>
        
        <p>and that of <strong>citation_influence_label</strong> are:
        
        <ul>
          <li>0 - INCIDENTAL</li>
          <li>1 - INFLUENTIAL</li>
        </ul>
        
        </p>
        <p>The following table shows a sample entry from the training dataset.</p>
        
        <table class="table table-striped">
          <tr>
            <td>unique_id</td>
            <td>1998</td>
          </tr>
          <tr>
            <td>core_id</td>
            <td>81605842</td>
          </tr>
          <tr>
            <td>citing_title</td>
            <td>Everolimus improves behavioral deficits in a patient with autism associated with tuberous
              sclerosis: a case report</td>
          </tr>
          <tr>
            <td>citing_author</td>
            <td>Ryouhei Ishii</td>
          </tr>
          <tr>
            <td>cited_title</td>
            <td>Learning disability and epilepsy in an epidemiological sample of individuals with tuberous
              sclerosis complex</td>
          </tr>
          <tr>
            <td>cited_author</td>
            <td>Joinson</td>
          </tr>
          <tr>
            <td>citation_context</td>
            <td>West syndrome (infantile spasms) is the common estc epileptic disorder, which is associated
              with more intellectual disability and a less favorable neurological outcome (#AUTHOR_TAG et al, 2003)</td>
          </tr>
          <tr>
            <td>citation_class_label</td>
            <td>4</td>
          </tr>
          <tr>
            <td>citation_influence_label</td>
            <td>1</td>
          </tr>
        </table>

        <br />

        <p>A sample training dataset can be downloaded by filling the <a href="https://forms.gle/AjYfMrTzZXjfBjgS6">shared task registration form</a>. The full training dataset will be released shortly via the <a href="https://www.kaggle.com/">Kaggle platform</a>.</p>

        <p>The <a href="http://jurgens.people.si.umich.edu/citation-function/" target="_blank">ACL-ARC</a> dataset <a
            href="#references">[4]</a>, which is compatible with our ACT dataset can be used by the participants during the
          competition.</p>
        
        <h3>Evaluation</h3>

        <p>The evaluation will be conducted using the withheld test data containing 1000 instances. The evaluation metric used
          will be the F1-macro score.</p>
        
        $$\mbox{F1-macro} = {\frac{1}{n} \sum_{i=1}^{n}{\frac{2 \times P_i \times R_i}{P_i + R_i}}}$$
        
        <h3 id="registration">Team Registration</h3>

        <p>Participants of the shared task need to register their team using <a href="https://forms.gle/AjYfMrTzZXjfBjgS6">this registration form</a>.</p>
        
        <!-- <p>This year, we are hosting the shared task on the <a href="https://www.kaggle.com/c/about/inclass"
            target="_blank">Kaggle inClass</a> platform. Please note that both subtasks will be hosted as separate competitions
          on kaggle. Please make sure you sign in/register to kaggle before clicking the following links.</p>
        
        <p>To participate in the Shared Task:</p>
        
        <dl class="topics">
          <dd>For subtask A, please visit <a href="https://www.kaggle.com/c/3c-shared-task-purpose/" target="_blank">Kaggle
              citation purpose </a>classification</dd>
          <dd>For subtask B, please visit <a href="https://www.kaggle.com/c/3c-shared-task-influence/" target="_blank">Kaggle
              citation influence </a>classification</dd>
        </dl> -->

        <!-- <p>
          To address the problem of all citations being treated equally by research metrics, we will run a shared task that will encourage the development of models capable of classifying citations according to their <i>influence</i> and <i>purpose</i>. The shared task will build on our experience from organizing the <a href="https://wosp.core.ac.uk/jcdl2020/shared_task.html#3ctask">3C shared task at WOSP 2020</a>, as well as the <a href="https://biendata.com/competition/wsdm2020/">Citation Intent Classification challenge</a> at WSDM 2020.
        </p>

        <p>
          Three existing datasets will be used: 
        </p>

        <ul>
          <li>the <a href="http://oro.open.ac.uk/70520/">ACT dataset</a> (over 11 thousand author annotated citation contexts),</li>
          <li><a href="https://dl.acm.org/doi/abs/10.1145/2740908.2742839">Microsoft Academic Graph</a> (a dataset of scientific publication records with citation links and contexts) and</li>
          <li>the <a href="http://oro.open.ac.uk/55987/">TrueImpactDataset</a> (a dataset of research papers annotated as seminal or survey).</li>
        </ul>

        <p>
          In addition to subtasks 1 and 2 introduced in 3C 2020, we will include a new subtask focused on classification of publications into seminal and survey categories according to the context in which they are cited. 3C 2020 saw 3 teams participate in subtask 1 and 4 teams in subtask 2, while WSDM Cup 2020 saw 99 teams participate in the challenge.
        </p> -->

        <h3>Organizers</h3>

        <p><a href="http://kmi.open.ac.uk/people/member/petr-knoth">Petr Knoth</a>, Open University, UK</p>

        <p><a href="http://kmi.open.ac.uk/people/member/suchetha-n-kunnath">Suchetha N. Kunnath</a>, Open University, UK</p>

        <p><a href="http://kmi.open.ac.uk/people/member/david-pride">David Pride</a>, Open University, UK</p>

        <p><a href="https://www.microsoft.com/en-us/research/people/kuansanw/">Kuansan Wang</a>, Microsoft Research</p>

        <h3>Contact</h3>

        <p>If you have any questions about this shared task, please contact <a href="mailto:david.pride@open.ac.uk">david.pride@open.ac.uk</a> and <a href="mailto:suchetha.nambanoor-kunnath@open.ac.uk">suchetha.nambanoor-kunnath@open.ac.uk</a>.</p>

        <!-- <p><a href="http://dasha.tech">Drahomira Herrmannova</a>, Oak Ridge National Laboratory</p> -->

        <h3>References</h3>

        <ol id="refs">
          <li>Knoth, Petr and Zdrahal, Zdenek. "<a href="http://www.dlib.org/dlib/november12/knoth/11knoth.html">CORE: three access levels to underpin open access</a>." D-Lib Magazine 18.11/12 (2012): 1-13.</li>
          <li>Pride, David and Knoth, Petr. "<a href="http://oro.open.ac.uk/51751/">Incidental or influential? &mdash; A decade of using text-mining for citation function classification</a>." (2017).</li>
          <li>Pride, David, Knoth, Petr and Harag, Jozef. "<a href="https://ieeexplore.ieee.org/abstract/document/8791134">ACT: An Annotation Platform for Citation Typing at Scale</a>." 2019 ACM/IEEE Joint Conference on Digital Libraries (JCDL). IEEE, 2019.</li>
          <li>Jurgens, David, et al. "<a href="https://www.mitpressjournals.org/doi/abs/10.1162/tacl_a_00028">Measuring the evolution of a scientific field through citation frames</a>." Transactions of the Association for Computational Linguistics 6 (2018): 391-406.</li>
        </ol>

        <!-- <hr class="featurette-divider"> -->

        <!-- IMPORTANT DATES ========================================================= -->

        <!-- <h2 id="register">Registration</h2>

        <p>
          To register for participation in the shared tasks, please use <a href="https://docs.google.com/forms/d/e/1FAIpQLScfHzByrog-k299qBuCp3SbPWcb905_kmOWMvHpDH57VLpVrg/viewform">this registration form</a>.
        </p>

        <p>TBA</p>

        <hr class="featurette-divider"> -->

        <!-- IMPORTANT DATES ========================================================= -->

        <!-- <h2 id="dates">Important Dates</h2>

        <p>
          Please consult the <a href="index.html">SDP Workshop website</a> for official dates for the workshop. All submission deadlines are 11:59 PM AoE (Anywhere on Earth) Time Zone (UTC-12).
        </p>

        <table class="table table-striped">
          <thead>
            <tr>
              <th scope="col">Event</th>
              <th scope="col">Date</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>Training Set Release</td>
              <td>Feb 15, 2020. An additional development set will be made available closer to the test set release date</td>
            </tr>
            <tr>
              <td>Deadline for Registration</td>
              <td> April 30 (remains open till evaluation window starts)<br /></td>
            </tr>
            <tr>
              <td>Test Set Release (Blind)</td>
              <td><del>July 1, 2020</del> July 15, 2020</td>
            </tr>
            <tr>
              <td>System Runs Due</td>
              <td><del>August 1, 2020</del> Aug 15, 2020 </td>
            </tr>
            <tr>
              <td>Preliminary System Reports Due in SoftConf</td>
              <td>August 16, 2020</td>
            </tr>
            <tr>
              <td>Camera-Ready Contributions Due in SoftConf</td>
              <td><del>August 31, 2020</del> Oct 10, 2020</td>
            </tr>
            <tr>
              <td>Participant Presentations at SDP 2020</td>
              <td><del>Nov 12</del> Nov 19, 2020</td>
            </tr>
          </tbody>
        </table>

        <hr class="featurette-divider"> -->

      </div>
    </div>

    <!-- FOOTER ========================================== -->

    <hr><br />

    <footer>
      <div class="footer-wrapper">
        <div class="footer-left">
          <p>Contact: <a href="mailto:sdproc2021@googlegroups.com">sdproc2021@googlegroups.com</a></p>
          <p>Sign up for updates: <a href="https://groups.google.com/g/sdproc-updates">https://groups.google.com/g/sdproc-updates</a></p>
          <p>Follow us: <a href="https://twitter.com/sdproc">https://twitter.com/SDProc</a></p>
        </div>
        <div class="footer-right">
          <a href="#">Back to top</a>
        </div>
    </footer>

  </div>

  <!-- Bootstrap core JavaScript ================================================== -->
  <script src="./dist/js/bootstrap.min.js"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

</body>

</html>
