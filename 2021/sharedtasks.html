<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="description" content="Second Workshop on Scholarly Document Processing">

  <title>2nd Workshop on Scholarly Document Processing</title>

  <!-- Bootstrap core CSS -->
  <link href="./dist/css/bootstrap.min.css" rel="stylesheet">

  <!-- Fira Sans font -->
  <link href="https://fonts.googleapis.com/css?family=Fira+Sans&display=swap" rel="stylesheet">

  <!--[if lt IE 9]><script src="../../assets/js/ie8-responsive-file-warning.js"></script><![endif]-->

  <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
  <!--[if lt IE 9]>
      <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
      <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->

  <!-- Custom styles for this template -->
  <link href="styles.css" rel="stylesheet">

  <!-- icons -->
  <link rel="stylesheet" href="./font-awesome-4.1.0/css/font-awesome.min.css">

  <!-- jQuery -->
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.12.4/jquery.min.js"></script>

</head>

<body>

  <!-- NAVBAR ================================================== -->

  <div class="navbar-wrapper"></div>
  <script src="menu.js" type="text/javascript"></script>

  <!-- MAIN CONTENT ============================================= -->

  <div class="container marketing navbar-spacing">

    <div class="row">
      <div class="col-md-12">

        <!-- CFP INTRODUCTION ================================================== -->

        <!-- <h1>The 6<sup>th</sup> Computational Linguistics Scientific Document Summarization Shared
          Task (CL-SciSumm 2020)</h1> -->

        <h1>Shared Tasks: Call for Participation</h1>

        <!-- <p>
          <a href="https://docs.google.com/forms/d/e/1FAIpQLScfHzByrog-k299qBuCp3SbPWcb905_kmOWMvHpDH57VLpVrg/viewform"><button type="button" class="btn btn-primary">Shared Tasks Registration</button></a>
        </p>

        <hr class="featurette-divider">

        <h2>Navigation</h2>

        <ul>
          <li>
            <a href="#clscisumm">CL-SciSumm 2020</a>
          </li>
          <li>
            <a href="#laysumm">CL-LaySumm 2020</a>
          </li>
          <li>
            <a href="#longsumm">LongSumm 2020</a>
          </li>
          <li>
            <a href="#register">Registration</a>
          </li>
          <li>
            <a href="#dates">Important Dates</a>
          </li>
          <li>
            <a href="#organizers">Organizing Committee</a>
          </li>
        </ul> -->

        <hr class="featurette-divider">

        <!-- LongSumm ========================================================= -->

        <h2 id="longsumm">LongSumm 2021: The 2<sup>nd</sup> Shared Task on Generating Long Summaries for Scientific
          Documents</h2>

        <p>
          The LongSumm Shared Task focuses on generating long summaries of scientific articles which require expertise
          in a scientific domain. To this end, we created a training set that consists of 1,705 extractive summaries,
          and 700 abstractive summaries of NLP and Machine Learning scientific papers. These are drawn from papers based
          on video talks from associated conferences (<a href="https://www.aclweb.org/anthology/P19-1204/">TalkSumm</a>)
          and from blogs created by NLP and ML researchers. Each submission is judged against one reference summary
          (gold summary) on ROUGE and should not exceed 600 words.
        </p>

        <p>In SDP 2020, LongSumm received 100 submissions from 10 different teams. Evaluation results are reported on a
          <a href="https://aieval.draco.res.ibm.com/challenge/39/leaderboard/39">public leaderboard</a>. In 2021, the
          task will continue to expand, by incorporating additional summaries.</p>

        <!-- <p>
          Most of the work on scientific document summarization focuses on generating relatively short summaries (abstract like). While such a length constraint can be sufficient for summarizing news articles, it is far from sufficient for summarizing scientific work. In fact, such a short summary resembles more to an abstract than to a summary that aims to cover all the salient information conveyed in a given text. Writing such summaries requires expertise and a deep understanding in a scientific domain, as can be found in some researchers blogs.
        </p>

        <p>
          The LongSumm task opted to leverage blogs created by researchers in the NLP and Machine learning communities and use these summaries as reference summaries to compare the submissions against.
        </p>

        <p>
          The corpus for this task includes a training set that consists of 1,705 extractive summaries, and around 700 abstractive summaries of NLP and Machine Learning scientific papers. These are drawn from papers based on video talks from associated conferences (<a href="https://arxiv.org/abs/1906.01351">Lev et al. 2019 TalkSumm</a>) and from blogs created by NLP and ML researchers. In addition, we create a test set of abstractive summaries. Each submission is judged against one reference summary (gold summary) on ROUGE and should not exceed 600 words.
        </p>

        <h3>Long Summary Task</h3>

        <p>
          The task is defined as follows:
        </p>

        <ul>
          <li>
            <strong>Given:</strong> For a detailed description of the provided data, please see the <a href="https://github.com/guyfe/LongSumm#training-data">LongSumm GitHub repository</a>
          </li>
          <li>
            <strong>Task:</strong> Generate abstractive and extractive summaries for scientific papers
          </li>
        </ul>

        <h4>Evaluation</h4>

        <p>
          The Long Summary Task will be scored by using several ROUGE metrics to compare the system output and the gold standard Lay Summary. The intrinsic evaluation will be done by ROUGE, using ROUGE-1, -2, -L and Skipgram metrics. In addition, a randomly selected subset of the summaries will undergo human evaluation.
        </p>

        <h3>Corpus</h3>

        <p>
          The training data is composed of abstractive and extractive summaries. To download both datasets, and for further details, see the <a href="https://github.com/guyfe/LongSumm#training-data">LongSumm GitHub repository</a>.
        </p>

        <p>
          The (blind) test dataset will be released on July 15, 2020.
        </p>-->

        <h3>Organizers</h3>

        <p><a href="https://researcher.watson.ibm.com/researcher/view.php?person=il-GUYF">Guy Feigenblat</a>, IBM Research AI</p>

        <p><a href="https://researcher.watson.ibm.com/researcher/view.php?person=il-SHMUELI">Michal Shmueli-Scheuer</a>, IBM Research AI</p>

        <hr class="featurette-divider">

        <!-- SciFact ========================================================= -->

        <h2 id="scifact">Shared Task on Rationalized Verification of Scientific Claims</h2>

        <p>
          To help scientists with information overload and support scientific fact checking and evidence synthesis, 
          we aim to build systems that can take an input claim, identify all relevant Supporting and Refuting text 
          excerpts from papers in a large corpus, and provide the rationales for those predictions. In this shared task, 
          we will use the <a href="https://arxiv.org/abs/2004.14974">SciFact
            dataset</a> dataset of 1.4K expert-annotated biomedical claims verified against a large corpus of peer-reviewed papers. 
            The <a href="https://scifact.apps.allenai.org/leaderboard">public leaderboard</a> already has submissions making progress on this task.
          We will augment a subset of the SciFact corpus with additional annotations of <a href="https://arxiv.org/abs/1702.05398">
              scientific discourse segments</a>, which will serve as a more detailed form of rationales for verified claims, and 
          can potentially help identify new claims as well.
        </p>

        <h3>Organizers</h3>

        <!-- <p><a href="http://armancohan.com/">Arman Cohan</a>, Allen Institute for Artificial Intelligence (AI2)</p> -->

        <p><a href="https://kyleclo.github.io/">Kyle Lo</a>, Allen Institute for Artificial Intelligence (AI2)</p>

        <p><a href="https://beltagy.net/">Iz Beltagy</a>, Allen Institute for Artificial Intelligence (AI2)</p>

        <p><a href="https://www.linkedin.com/in/anitadewaard/">Anita de Waard</a>, Elsevier, USA</p>

        <p><a href="https://tirthankarslg.wixsite.com/ainlpmldl">Tirthankar Ghosal</a>, Indian Institute of Technology Patna, India</p>

        <p><a href="http://kmi.open.ac.uk/people/member/petr-knoth">Petr Knoth</a>, The Open University, UK</p>

        <!-- <p><a href="https://llwang.net/">Lucy Lu Wang</a>, Allen Institute for Artificial Intelligence (AI2)</p> -->

        <hr class="featurette-divider">

        <!-- 3C ======================================================== -->

        <h2 id="3c">3C Citation Context Classification</h2>

        <p>
          To address the problem of all citations being treated equally by research metrics, we will run a shared task that will encourage the development of models capable of classifying citations according to their <i>influence</i> and <i>purpose</i>. The shared task will build on our experience from organizing the <a href="https://wosp.core.ac.uk/jcdl2020/shared_task.html#3ctask">3C shared task at WOSP 2020</a>, as well as the <a href="https://biendata.com/competition/wsdm2020/">Citation Intent Classification challenge</a> at WSDM 2020.
        </p>

        <p>
          Three existing datasets will be used: 
        </p>

        <ul>
          <li>the <a href="http://oro.open.ac.uk/70520/">ACT dataset</a> (over 11 thousand author annotated citation contexts),</li>
          <li><a href="https://dl.acm.org/doi/abs/10.1145/2740908.2742839">Microsoft Academic Graph</a> (a dataset of scientific publication records with citation links and contexts) and</li>
          <li>the <a href="http://oro.open.ac.uk/55987/">TrueImpactDataset</a> (a dataset of research papers annotated as seminal or survey).</li>
        </ul>

        <p>
          In addition to subtasks 1 and 2 introduced in 3C 2020, we will include a new subtask focused on classification of publications into seminal and survey categories according to the context in which they are cited. 3C 2020 saw 3 teams participate in subtask 1 and 4 teams in subtask 2, while WSDM Cup 2020 saw 99 teams participate in the challenge.
        </p>

        <h3>Organizers</h3>

        <p><a href="http://kmi.open.ac.uk/people/member/petr-knoth">Petr Knoth</a>, Open University, UK</p>

        <p><a href="https://www.microsoft.com/en-us/research/people/kuansanw/">Kuansan Wang</a>, Microsoft Research</p>

        <p><a href="http://dasha.tech">Drahomira Herrmannova</a>, Oak Ridge National Laboratory</p>

        <!-- <hr class="featurette-divider"> -->

        <!-- IMPORTANT DATES ========================================================= -->

        <!-- <h2 id="register">Registration</h2>

        <p>
          To register for participation in the shared tasks, please use <a href="https://docs.google.com/forms/d/e/1FAIpQLScfHzByrog-k299qBuCp3SbPWcb905_kmOWMvHpDH57VLpVrg/viewform">this registration form</a>.
        </p>

        <p>TBA</p>

        <hr class="featurette-divider"> -->

        <!-- IMPORTANT DATES ========================================================= -->

        <!-- <h2 id="dates">Important Dates</h2>

        <p>
          Please consult the <a href="index.html">SDP Workshop website</a> for official dates for the workshop. All submission deadlines are 11:59 PM AoE (Anywhere on Earth) Time Zone (UTC-12).
        </p>

        <table class="table table-striped">
          <thead>
            <tr>
              <th scope="col">Event</th>
              <th scope="col">Date</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>Training Set Release</td>
              <td>Feb 15, 2020. An additional development set will be made available closer to the test set release date</td>
            </tr>
            <tr>
              <td>Deadline for Registration</td>
              <td> April 30 (remains open till evaluation window starts)<br /></td>
            </tr>
            <tr>
              <td>Test Set Release (Blind)</td>
              <td><del>July 1, 2020</del> July 15, 2020</td>
            </tr>
            <tr>
              <td>System Runs Due</td>
              <td><del>August 1, 2020</del> Aug 15, 2020 </td>
            </tr>
            <tr>
              <td>Preliminary System Reports Due in SoftConf</td>
              <td>August 16, 2020</td>
            </tr>
            <tr>
              <td>Camera-Ready Contributions Due in SoftConf</td>
              <td><del>August 31, 2020</del> Oct 10, 2020</td>
            </tr>
            <tr>
              <td>Participant Presentations at SDP 2020</td>
              <td><del>Nov 12</del> Nov 19, 2020</td>
            </tr>
          </tbody>
        </table>

        <hr class="featurette-divider"> -->

      </div>
    </div>

    <!-- FOOTER ========================================== -->

    <hr><br />

    <footer>
      <div class="footer-wrapper">
        <div class="footer-left">
          <p>Contact: <a href="mailto:sdproc2021@googlegroups.com">sdproc2021@googlegroups.com</a></p>
          <p>Sign up for updates: <a href="https://groups.google.com/g/sdproc-updates">https://groups.google.com/g/sdproc-updates</a></p>
          <p>Follow us: <a href="https://twitter.com/sdproc">https://twitter.com/SDProc</a></p>
        </div>
        <div class="footer-right">
          <a href="#">Back to top</a>
        </div>
    </footer>

  </div>

  <!-- Bootstrap core JavaScript ================================================== -->
  <script src="./dist/js/bootstrap.min.js"></script>

</body>

</html>